{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reference, [click here](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model?view=azureml-api-2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prerequisites "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1685486329865
        }
      },
      "outputs": [],
      "source": [
        "# ! pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1687967218052
        }
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    OnlineRequestSettings\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1687967221645
        }
      },
      "outputs": [],
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "resource_group = \"<RESOURCE_GROUP>\"\n",
        "workspace = \"<AML_WORKSPACE_NAME>\"\n",
        "\n",
        "# get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687967914375
        }
      },
      "outputs": [],
      "source": [
        "online_endpoint_name = \"padchest-optimized-ipex-ov-sdk-v2\"\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name = online_endpoint_name, \n",
        "    description=\"online deployment of: padchest-ipex-sdk-v2-2\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "\n",
        "poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
        "poller.wait()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687967941258
        }
      },
      "outputs": [],
      "source": [
        "# Configure a model\n",
        "\n",
        "\n",
        "folder_model_path=\"./outputs/az-register-models\"\n",
        "\n",
        "file_model = Model(\n",
        "    path=folder_model_path,\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=\"padchest-opti-sdk-v2-endpoint\",\n",
        "    version=\"1\",\n",
        "    description=\"SDKv2-az-register-models with PT, ONNX and OV models of padchest\"\n",
        ")\n",
        "ml_client.models.create_or_update(file_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1687970910761
        }
      },
      "outputs": [],
      "source": [
        "# Configure an environment\n",
        "\n",
        "env = Environment(\n",
        "    conda_file=\"conda_dep_opti.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    )\n",
        "\n",
        "\n",
        "# configure an inference configuration with a scoring script\n",
        "code_config = CodeConfiguration(\n",
        "        code=\"padchest_score_code\",\n",
        "        scoring_script=\"score_opti.py\"\n",
        "    )   "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Deployment\n",
        "See VM SKUs that are supported for Azure Machine Learning managed online endpoints [here](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list?view=azureml-api-2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Deployment\n",
        "See VM SKUs that are supported for Azure Machine Learning managed online endpoints [here](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list?view=azureml-api-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687970919047
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "req_settings = OnlineRequestSettings(request_timeout_ms=90000)\n",
        "\n",
        "# Define a deployment\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=file_model,\n",
        "    environment=env,\n",
        "    code_configuration=code_config,\n",
        "    instance_type=\"Standard_FX4mds\", #Standard_FX12mds, #Standard_FX24mds \n",
        "    instance_count=1,\n",
        "    request_settings=req_settings\n",
        ")\n",
        "\n",
        "# create the deployment:\n",
        "poller = ml_client.begin_create_or_update(blue_deployment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687971166179
        }
      },
      "outputs": [],
      "source": [
        "# blue deployment takes 100% traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1685993900732
        }
      },
      "outputs": [],
      "source": [
        "deployment_logs = ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
        ")\n",
        "deployment_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687971197115
        }
      },
      "outputs": [],
      "source": [
        "# Get the details for online endpoint\n",
        "deployed_endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(deployed_endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(deployed_endpoint.scoring_uri)\n",
        "\n",
        "auth_key = ml_client.online_endpoints.get_keys(online_endpoint_name).primary_key\n",
        "print(f\"Authkye:{auth_key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687968719552
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# visualize image\n",
        "\n",
        "import pydicom\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Visualize converted DICOM file from the corresponding PNG file\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "dcm = pydicom.read_file(test_file)\n",
        "print(dcm)\n",
        "plt.imshow(dcm.pixel_array, cmap=plt.cm.bone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1687971236518
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "files = {'image': open(test_file, 'rb').read()}\n",
        "\n",
        "# resp = requests.post(scoring_uri, input_data, headers=headers)\n",
        "scoring_uri = deployed_endpoint.scoring_uri\n",
        "\n",
        "# Send the DICOM as a raw HTTP request and obtain results from endpoint.\n",
        "response = requests.post(scoring_uri, headers={\"Authorization\": f\"Bearer {auth_key}\"},files=files, timeout=60)\n",
        "print(\"output:\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1687974788821
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock PyTorch Metrics:\n",
            "\tFramework Version:\tPyTorch: 1.13.1+cpu\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0506 sec\n",
            "\tFPS:\t19.76\n",
            "PyTorch Graph Mode Metrics:\n",
            "\tFramework Version:\tPyTorch: 1.13.1+cpu\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0429 sec\n",
            "\tFPS:\t23.32\n",
            "\n",
            "IPEX Eager Metrics:\n",
            "\tFramework Version:\tIPEX: 1.13.100\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0585 sec\n",
            "\tFPS:\t17.09\n",
            "\n",
            "IPEX Graph Mode Metrics:\n",
            "\tFramework Version:\tIPEX: 1.13.100\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0273 sec\n",
            "\tFPS:\t36.69\n",
            "\n",
            "OpenVINO Metrics:\n",
            "\tFramework Version:\tOpenVINO: 2023.0.0-10926-b4452d56304-releases/2023/0\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0208 sec\n",
            "\tFPS:\t48.10\n",
            "\n",
            "Speedup with IPEX: 1.86x\n",
            "\n",
            "Speedup with OV: 2.43x\n",
            "\n",
            "Speedup with stock graph mode: 1.18x\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "output_dict = json.loads(response.content)\n",
        "\n",
        "pt_metrics = output_dict['pt_summary']\n",
        "pt_graph_metrics = output_dict['pt_graph_summary']\n",
        "ipex_metrics = output_dict['ipex_eager_summary']\n",
        "ipex_graph_metrics = output_dict['ipex_graph_summary']\n",
        "ov_metrics = output_dict['ov_summary']\n",
        "\n",
        "print(f\"Stock PyTorch Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{pt_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{pt_metrics['pt_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{pt_metrics['pt_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{pt_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{pt_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"PyTorch Graph Mode Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{pt_graph_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{pt_graph_metrics['pt_graph_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{pt_graph_metrics['pt_graph_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{pt_graph_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{pt_graph_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nIPEX Eager Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{ipex_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{ipex_metrics['ipex_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ipex_metrics['ipex_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ipex_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ipex_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nIPEX Graph Mode Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{ipex_graph_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{ipex_graph_metrics['ipex_graph_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ipex_graph_metrics['ipex_graph_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ipex_graph_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ipex_graph_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nOpenVINO Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{ov_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{ov_metrics['ov_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ov_metrics['ov_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ov_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ov_metrics['fps']:.2f}\")\n",
        "\n",
        "\n",
        "# Calculate the FPS speedup with IPEX compared to PyTorch\n",
        "ipex_fps_speedup = ipex_graph_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with IPEX: {ipex_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the FPS speedup with OpenVINO compared to PyTorch\n",
        "ov_fps_speedup = ov_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with OV: {ov_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the FPS speedup with Stock Graph Mode compared to PyTorch\n",
        "ov_fps_speedup = pt_graph_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with stock graph mode: {ov_fps_speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1687974811038
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System Info:\n",
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Byte Order:                      Little Endian\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "CPU(s):                          4\n",
            "On-line CPU(s) list:             0-3\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              2\n",
            "Socket(s):                       1\n",
            "NUMA node(s):                    1\n",
            "Vendor ID:                       GenuineIntel\n",
            "CPU family:                      6\n",
            "Model:                           85\n",
            "Model name:                      Intel(R) Xeon(R) Gold 6246R CPU @ 3.40GHz\n",
            "Stepping:                        7\n",
            "CPU MHz:                         3392.031\n",
            "BogoMIPS:                        6784.06\n",
            "Virtualization:                  VT-x\n",
            "Hypervisor vendor:               Microsoft\n",
            "Virtualization type:             full\n",
            "L1d cache:                       64 KiB\n",
            "L1i cache:                       64 KiB\n",
            "L2 cache:                        2 MiB\n",
            "L3 cache:                        35.8 MiB\n",
            "NUMA node0 CPU(s):               0-3\n",
            "Vulnerability Itlb multihit:     Not affected\n",
            "Vulnerability L1tf:              Not affected\n",
            "Vulnerability Mds:               Not affected\n",
            "Vulnerability Meltdown:          Not affected\n",
            "Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
            "Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single tpr_shadow vnmi ept vpid ept_ad fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_vnni arch_capabilities\n",
            "\n",
            "\n",
            "System Memory Info (GB):\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:             82           2          73           0           7          79\n",
            "Swap:             0           0           0\n",
            "\n",
            "\n",
            "System OS:\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"20.04.6 LTS (Focal Fossa)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\n",
            "VERSION_ID=\"20.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=focal\n",
            "UBUNTU_CODENAME=focal\n",
            " \n",
            "Linux mir-user-pod-94e28fde802543ea8d41a8bf502bf8c1000001 5.15.0-1038-azure #45~20.04.1-Ubuntu SMP Tue Apr 25 18:45:15 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n",
            "\n",
            "/azureml-envs/azureml_06a7f6bafdc80fc3c451c149a3ddc83a/bin/python\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Print System info\n",
        "lscpu_out=output_dict['system_info']['lscpu_out'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Info:\\n{lscpu_out}\")\n",
        "\n",
        "mem_out_gb=output_dict['system_info']['mem_out_gb'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Memory Info (GB):\\n{mem_out_gb}\")\n",
        "\n",
        "os_out=output_dict['system_info']['os'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem OS:\\n{os_out}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".."
          ]
        }
      ],
      "source": [
        "#ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
