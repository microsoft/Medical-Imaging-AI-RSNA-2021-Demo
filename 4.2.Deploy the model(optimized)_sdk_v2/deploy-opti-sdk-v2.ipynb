{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy a model to an online endpoint, using Azure Machine Learning Python SDK v2."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reference, [click here](https://learn.microsoft.com/en-us/azure/machine-learning/tutorial-deploy-model?view=azureml-api-2)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prerequisites "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install azure-ai-ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    OnlineRequestSettings\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# enter details of your AML workspace\n",
        "subscription_id = \"<SUBSCRIPTION_ID>\"\n",
        "resource_group = \"<RESOURCE_GROUP>\"\n",
        "workspace_name = \"<AML_WORKSPACE_NAME>\"\n",
        "\n",
        "# authenticate\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=subscription_id,\n",
        "    resource_group_name=resource_group,\n",
        "    workspace_name=workspace_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure an environment\n",
        "\n",
        "env = Environment(\n",
        "    conda_file=\"conda_dep_opti.yml\",\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",\n",
        "    )\n",
        "\n",
        "# configure an inference configuration with a code folder and scoring script\n",
        "code_config = CodeConfiguration(\n",
        "        code=\"padchest_score_code\",\n",
        "        scoring_script=\"score_opti.py\"\n",
        "    )   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure/Register a model in AzureML\n",
        "\n",
        "model_path=\"../outputs/az-register-models\"\n",
        "\n",
        "model = Model(\n",
        "    path=model_path,\n",
        "    type=AssetTypes.CUSTOM_MODEL,\n",
        "    name=\"padchest-pt-onnx-ov-v2sdk\",\n",
        "    version=\"1\",\n",
        "    description=\"SDKv2-az-register-models with PT, ONNX and OV models of padchest\"\n",
        ")\n",
        "ml_client.models.create_or_update(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "endpoint_name = \"padchest-opti-endpt-sdk-v2\"\n",
        "# Define and setup an endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name = endpoint_name, \n",
        "    description=\"padchest-opti-sdk-v2-endpoint\",\n",
        "    auth_mode=\"key\"\n",
        ")\n",
        "# create an ONLINE endpoint\n",
        "poller = ml_client.online_endpoints.begin_create_or_update(endpoint)\n",
        "poller.wait()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Deployment\n",
        "See VM SKUs that are supported for Azure Machine Learning managed online endpoints [here](https://learn.microsoft.com/en-us/azure/machine-learning/reference-managed-online-endpoints-vm-sku-list?view=azureml-api-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Check: endpoint padchest-opti-endpt-sdk-v2 exists\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "............................................................................................"
          ]
        }
      ],
      "source": [
        "\n",
        "req_settings = OnlineRequestSettings(request_timeout_ms=36000)\n",
        "\n",
        "# Define a deployment\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    endpoint_name=endpoint_name,\n",
        "    model=model,\n",
        "    environment=env,\n",
        "    code_configuration=code_config,\n",
        "    instance_type=\"Standard_F2s_v2\", #Standard_FX4mds, Standard_F2s_v2\n",
        "    instance_count=1,\n",
        "    request_settings=req_settings\n",
        ")\n",
        "\n",
        "# create the deployment:\n",
        "poller = ml_client.begin_create_or_update(blue_deployment)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<azure.core.polling._poller.LROPoller at 0x7fad5c75eb80>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# blue deployment takes 100% traffic\n",
        "endpoint.traffic = {\"blue\": 100}\n",
        "ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "deployment_logs = ml_client.online_deployments.get_logs(\n",
        "    name=\"blue\", endpoint_name=endpoint_name, lines=50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'blue': 100}\n",
            "https://padchest-opti-endpt-sdk-v2.eastus.inference.ml.azure.com/score\n",
            "Authkye:m3C7n2b1kYVwDiDfiJj9LA42d1lRBTLQ\n"
          ]
        }
      ],
      "source": [
        "# Get the details for online endpoint\n",
        "deployed_endpoint = ml_client.online_endpoints.get(name=endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(deployed_endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(deployed_endpoint.scoring_uri)\n",
        "\n",
        "auth_key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
        "print(f\"Authkye:{auth_key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "files = {'image': open(test_file, 'rb').read()}\n",
        "\n",
        "# resp = requests.post(scoring_uri, input_data, headers=headers)\n",
        "scoring_uri = deployed_endpoint.scoring_uri\n",
        "\n",
        "# Send the DICOM as a raw HTTP request and obtain results from endpoint.\n",
        "response = requests.post(scoring_uri, headers={\"Authorization\": f\"Bearer {auth_key}\"},files=files, timeout=60)\n",
        "print(\"output:\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Metrics:\n",
            "\tFramework Version:\t1.13.1+cpu\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0875 sec\n",
            "\tFPS:\t11.43\n",
            "\n",
            "IPEX Metrics:\n",
            "\tFramework Version:\t1.13.100\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0687 sec\n",
            "\tFPS:\t14.55\n",
            "\n",
            "OpenVINO Metrics:\n",
            "\tFramework Version:\t2022.3.0-9052-9752fafe8eb-releases/2022/3\n",
            "\tTop Labels:\t['Pneumonia', 'Infiltration', 'Effusion']\n",
            "\tTop Probabilities:\t[49.63, 32.22, 3.29]\n",
            "\tAvg Latency:\t0.0444 sec\n",
            "\tFPS:\t22.53\n",
            "\n",
            "Speedup with IPEX: 1.27x\n",
            "\n",
            "Speedup with OpenVINO: 1.97x\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "output_dict = json.loads(response.content)\n",
        "\n",
        "pt_metrics = output_dict['pt_summary']\n",
        "ipex_metrics = output_dict['ipex_summary']\n",
        "ov_metrics = output_dict['ov_summary']\n",
        "\n",
        "print(f\"PyTorch Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['PyTorch']}\")\n",
        "print(f\"\\tTop Labels:\\t{pt_metrics['pt_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{pt_metrics['pt_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{pt_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{pt_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nIPEX Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['IPEX']}\")\n",
        "print(f\"\\tTop Labels:\\t{ipex_metrics['ipex_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ipex_metrics['ipex_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ipex_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ipex_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nOpenVINO Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{output_dict['system_info']['fwk_versions']['OpenVINO']}\")\n",
        "print(f\"\\tTop Labels:\\t{ov_metrics['ov_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ov_metrics['ov_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ov_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ov_metrics['fps']:.2f}\")\n",
        "\n",
        "# Calculate the FPS speedup with IPEX compared to PyTorch\n",
        "ipex_fps_speedup = ipex_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with IPEX: {ipex_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the FPS speedup with OpenVINO compared to PyTorch\n",
        "ov_fps_speedup = ov_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with OpenVINO: {ov_fps_speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "System Info:\n",
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Byte Order:                      Little Endian\n",
            "Address sizes:                   46 bits physical, 48 bits virtual\n",
            "CPU(s):                          2\n",
            "On-line CPU(s) list:             0,1\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              1\n",
            "Socket(s):                       1\n",
            "NUMA node(s):                    1\n",
            "Vendor ID:                       GenuineIntel\n",
            "CPU family:                      6\n",
            "Model:                           85\n",
            "Model name:                      Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n",
            "Stepping:                        7\n",
            "CPU MHz:                         2593.905\n",
            "BogoMIPS:                        5187.81\n",
            "Virtualization:                  VT-x\n",
            "Hypervisor vendor:               Microsoft\n",
            "Virtualization type:             full\n",
            "L1d cache:                       32 KiB\n",
            "L1i cache:                       32 KiB\n",
            "L2 cache:                        1 MiB\n",
            "L3 cache:                        35.8 MiB\n",
            "NUMA node0 CPU(s):               0,1\n",
            "Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\n",
            "Vulnerability L1tf:              Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\n",
            "Vulnerability Mds:               Mitigation; Clear CPU buffers; SMT Host state unknown\n",
            "Vulnerability Meltdown:          Mitigation; PTI\n",
            "Vulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n",
            "Vulnerability Retbleed:          Vulnerable\n",
            "Vulnerability Spec store bypass: Vulnerable\n",
            "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
            "Vulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\n",
            "Vulnerability Srbds:             Not affected\n",
            "Vulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT Host state unknown\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti tpr_shadow vnmi ept vpid fsgsbase bmi1 hle avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap clflushopt avx512cd avx512bw avx512vl xsaveopt xsavec xsaves md_clear\n",
            "\n",
            "\n",
            "System Memory Info (GB):\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:              3           1           0           0           2           2\n",
            "Swap:             0           0           0\n",
            "\n",
            "\n",
            "System OS:\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION=\"20.04.6 LTS (Focal Fossa)\"\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "PRETTY_NAME=\"Ubuntu 20.04.6 LTS\"\n",
            "VERSION_ID=\"20.04\"\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "VERSION_CODENAME=focal\n",
            "UBUNTU_CODENAME=focal\n",
            " \n",
            "Linux mir-user-pod-7807386888094ef3b3d916eb9839a15a000001 5.15.0-1037-azure #44~20.04.1-Ubuntu SMP Mon Apr 24 21:52:51 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux\n",
            "\n",
            "/azureml-envs/azureml_38ee30f8334103defc68160aec2da9ac/bin/python\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Print System info\n",
        "lscpu_out=output_dict['system_info']['lscpu_out'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Info:\\n{lscpu_out}\")\n",
        "\n",
        "mem_out_gb=output_dict['system_info']['mem_out_gb'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Memory Info (GB):\\n{mem_out_gb}\")\n",
        "\n",
        "os_out=output_dict['system_info']['os'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem OS:\\n{os_out}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Delete endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ".."
          ]
        }
      ],
      "source": [
        "#ml_client.online_endpoints.begin_delete(name=endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
