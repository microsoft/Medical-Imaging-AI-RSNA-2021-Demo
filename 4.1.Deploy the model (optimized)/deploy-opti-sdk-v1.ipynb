{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Deploy the model as a web service hosted on Azure Container Instances (ACI). \n",
        "\n",
        "1. Create the scoring script.\n",
        "1. Prepare an inference configuration.\n",
        "1. Deploy the previously trained model to the cloud.\n",
        "1. Consume data sample and test the web service."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  1. Create the scoring script.\n",
        "\n",
        "Create the scoring script, called score.py, used by the web service call to show how to use the model.  \n",
        "You must include two required functions into the scoring script:\n",
        "* The `init()` function, which typically loads the model into a global object. \n",
        "    * This function is run only once when the Docker container is started. \n",
        "* The `run(input_data)` function uses the model to predict a value based on the input data. \n",
        "    * Inputs and outputs to the run typically use JSON for serialization and de-serialization, but other formats are supported.\n",
        "\n",
        "TIP: Documentation on Deploy a model to Azure Container Instances [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance/). Advanced entry script authoring [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script#binary-data/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile score_opti.py\n",
        "from azureml.contrib.services.aml_request import AMLRequest, rawhttp\n",
        "from azureml.contrib.services.aml_response import AMLResponse\n",
        "import json, os, io\n",
        "import numpy as np\n",
        "import torch\n",
        "import intel_extension_for_pytorch as ipex\n",
        "import torchxrayvision as xrv\n",
        "from torchvision import transforms\n",
        "from torchxrayvision.datasets import normalize\n",
        "import pydicom\n",
        "\n",
        "import time\n",
        "from openvino.runtime import Core\n",
        "from openvino.runtime import get_version\n",
        "\n",
        "def init():\n",
        "    global bench_time\n",
        "    bench_time = 10  # benchmark time in sec\n",
        "    global target_device\n",
        "    target_device = \"CPU\"\n",
        "\n",
        "    # Initial PyTorch model\n",
        "    global modelx\n",
        "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
        "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
        "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
        "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'az-register-models', 'pc-densenet-densenet-best.pt')\n",
        "    # print(model_path)\n",
        "    modelx = torch.load(model_path)\n",
        "    modelx.eval()\n",
        "\n",
        "    # Initial PyTorch IPEX model\n",
        "    global ipex_modelx\n",
        "    global traced_model\n",
        "    ipex_modelx = ipex.optimize(modelx)\n",
        "\n",
        "    # Initialize OpenVINO Runtime.\n",
        "    global ov_compiled_model\n",
        "    ie = Core()\n",
        "    ov_xml = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'az-register-models', 'pc-densenet-densenet-best.onnx')\n",
        "    # Load and compile the OV model\n",
        "    ov_model = ie.read_model(ov_xml)\n",
        "    ov_compiled_model = ie.compile_model(model=ov_model, device_name=target_device)\n",
        "\n",
        "\n",
        "\n",
        "# TIP:  To accept raw data, use the AMLRequest class in your entry script and add the @rawhttp decorator to the run() function\n",
        "#       more details in: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
        "# Note that despite the fact that we trained our model on PNGs, we would like to simulate\n",
        "# a scenario closer to the real world here and accept DICOMs into our score script. Here's how:\n",
        "@rawhttp\n",
        "def run(request):\n",
        "\n",
        "    if request.method == 'GET':\n",
        "        # For this example, just return the URL for GETs.\n",
        "        respBody = str.encode(request.full_path)\n",
        "        return AMLResponse(respBody, 200)\n",
        "\n",
        "    elif request.method == 'POST':\n",
        "        # For a real-world solution, you would load the data from reqBody\n",
        "        # and send it to the model. Then return the response.\n",
        "        try:\n",
        "\n",
        "            # For labels definition see file: '3.Build a model/trainingscripts/padchest_config.py'\n",
        "            pathologies_labels = ['Air Trapping', 'Aortic Atheromatosis', 'Aortic Elongation', 'Atelectasis',\n",
        "             'Bronchiectasis', 'Cardiomegaly', 'Consolidation', 'Costophrenic Angle Blunting', 'Edema', 'Effusion',\n",
        "             'Emphysema', 'Fibrosis', 'Flattened Diaphragm', 'Fracture', 'Granuloma', 'Hemidiaphragm Elevation',\n",
        "             'Hernia', 'Hilar Enlargement', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n",
        "             'Pneumonia', 'Pneumothorax', 'Scoliosis', 'Tuberculosis']\n",
        "            def benchmark_pt(test_image):\n",
        "                latency_arr = []\n",
        "                end = time.time() + int(bench_time)\n",
        "\n",
        "                print(f\"\\n==== Benchmarking PyTorch inference with Fake Data for {bench_time}sec on CPU ====\")\n",
        "                print(f\"Input shape: {test_image.shape}\")\n",
        "\n",
        "                while time.time() < end:\n",
        "                    start_time = time.time()\n",
        "                    pt_result = modelx(test_image)\n",
        "                    latency = time.time() - start_time\n",
        "                    latency_arr.append(latency)\n",
        "\n",
        "                # Process output\n",
        "                index = np.argsort( pt_result.data.cpu().numpy() )\n",
        "                probability = torch.nn.functional.softmax(pt_result[0], dim=0).data.cpu().numpy()\n",
        "                pt_result = get_top_predictions(index, probability)\n",
        "\n",
        "                avg_latency = np.array(latency_arr).mean()\n",
        "                fps = 1 / avg_latency\n",
        "\n",
        "                print(f\"PyTorch Avg Latency: {avg_latency:.4f} sec, FPS: {fps:.2f}\")\n",
        "\n",
        "                #Return the result\n",
        "                pt_summary = {\n",
        "                    \"fwk_version\": f\"PyTorch: {torch.__version__}\",\n",
        "                    \"pt_result\": pt_result,\n",
        "                    \"avg_latency\": avg_latency,\n",
        "                    \"fps\": fps\n",
        "                }\n",
        "                return pt_summary\n",
        "\n",
        "            def benchmark_ipex(test_image):\n",
        "                latency_arr = []\n",
        "                end = time.time() + int(bench_time)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    traced_model = torch.jit.trace(ipex_modelx, test_image)\n",
        "                    traced_model = torch.jit.freeze(traced_model)\n",
        "\n",
        "                print(f\"\\n==== Benchmarking IPEX inference with Fake Data for {bench_time}sec on CPU ====\")\n",
        "                print(f\"Input shape: {test_image.shape}\")\n",
        "\n",
        "                while time.time() < end:\n",
        "                    start_time = time.time()\n",
        "                    with torch.no_grad():\n",
        "                        ipex_result = traced_model(test_image)\n",
        "                    latency = time.time() - start_time\n",
        "                    latency_arr.append(latency)\n",
        "\n",
        "                # Process output\n",
        "                index = np.argsort( ipex_result.data.cpu().numpy() )\n",
        "                probability = torch.nn.functional.softmax(ipex_result[0], dim=0).data.cpu().numpy()\n",
        "                ipex_result = get_top_predictions(index, probability)\n",
        "\n",
        "                avg_latency = np.array(latency_arr).mean()\n",
        "                fps = 1 / avg_latency\n",
        "\n",
        "                print(f\"PyTorch Avg Latency: {avg_latency:.4f} sec, FPS: {fps:.2f}\")\n",
        "\n",
        "                #Return the result\n",
        "                ipex_summary = {\n",
        "                    \"fwk_version\": f\"IPEX: {ipex.__version__}\",\n",
        "                    \"ipex_result\": ipex_result,\n",
        "                    \"avg_latency\": avg_latency,\n",
        "                    \"fps\": fps\n",
        "                }\n",
        "                return ipex_summary\n",
        "\n",
        "            def benchmark_ov(test_image):\n",
        "                # get the names of input and output layers of the model\n",
        "                input_layer = ov_compiled_model.input(0)\n",
        "                output_layer =ov_compiled_model.output(0)\n",
        "\n",
        "                latency_arr = []\n",
        "                end = time.time() + int(bench_time)\n",
        "                print(f\"\\n==== Benchmarking OpenVINO {bench_time}sec on {target_device} ====\")\n",
        "                print(f\"Input shape: {test_image.shape}\")\n",
        "\n",
        "                while time.time() < end:\n",
        "                    start_time = time.time()\n",
        "                    ov_output = ov_compiled_model([test_image])\n",
        "                    latency = time.time() - start_time\n",
        "                    latency_arr.append(latency)\n",
        "\n",
        "                # Process output\n",
        "                ov_output = ov_output[output_layer]\n",
        "                index = np.argsort(ov_output)\n",
        "                probability = torch.nn.functional.softmax(torch.from_numpy(ov_output[0]), dim=0).data.cpu().numpy()\n",
        "                ov_result = get_top_predictions(index, probability)\n",
        "\n",
        "                avg_latency = np.array(latency_arr).mean()\n",
        "                fps = 1 / avg_latency\n",
        "\n",
        "                print(f\"OpenVINO Avg Latency: {avg_latency:.4f} sec, FPS: {fps:.2f}\")\n",
        "\n",
        "                ov_summary = {\n",
        "                    \"fwk_version\": f\"OpenVINO: {get_version()}\",\n",
        "                    \"ov_result\": ov_result,\n",
        "                    \"avg_latency\": avg_latency,\n",
        "                    \"fps\": fps\n",
        "                }\n",
        "                return ov_summary\n",
        "\n",
        "            # Read DICOM and apply photometric transformations\n",
        "            def read_and_rescale_image( filepath):\n",
        "                dcm = pydicom.read_file(filepath)\n",
        "                image = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
        "\n",
        "                def window_image(image, wc, ww):\n",
        "                    img_min = wc - ww // 2\n",
        "                    img_max = wc + ww // 2\n",
        "                    image[image < img_min] = img_min\n",
        "                    image[image > img_max] = img_max\n",
        "                    return image\n",
        "\n",
        "                image = window_image(image, dcm.WindowCenter, dcm.WindowWidth)\n",
        "                # Scales 16bit to [-1024 1024]\n",
        "                image = normalize(image, maxval=65535, reshape=True)\n",
        "                return image\n",
        "\n",
        "            # Decode output and get predictions\n",
        "            def get_top_predictions(index, probability, num_predictions=3):\n",
        "                # For labels definition see file: '3.Build a model/trainingscripts/padchest_config.py'\n",
        "                pathologies_labels = ['Air Trapping', 'Aortic Atheromatosis', 'Aortic Elongation', 'Atelectasis',\n",
        "                    'Bronchiectasis', 'Cardiomegaly', 'Consolidation', 'Costophrenic Angle Blunting', 'Edema', 'Effusion',\n",
        "                    'Emphysema', 'Fibrosis', 'Flattened Diaphragm', 'Fracture', 'Granuloma', 'Hemidiaphragm Elevation',\n",
        "                    'Hernia', 'Hilar Enlargement', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n",
        "                    'Pneumonia', 'Pneumothorax', 'Scoliosis', 'Tuberculosis']\n",
        "\n",
        "                top_labels = []\n",
        "                top_probs = []\n",
        "                for i in range(num_predictions):\n",
        "                    top_labels.append(pathologies_labels[index[0][-1-i]])\n",
        "                    top_probs.append(round(probability[index[0][-1-i]] * 100, 2))\n",
        "\n",
        "                result = {\"top_labels\": top_labels, \"top_probabilities\": top_probs}\n",
        "                return result\n",
        "\n",
        "            # Get System information\n",
        "            def get_system_info():\n",
        "                import subprocess\n",
        "\n",
        "\n",
        "                # Run lscpu command and capture output\n",
        "                lscpu_out = subprocess.check_output(['lscpu']).decode('utf-8')\n",
        "                print(lscpu_out)\n",
        "                # Run free -g command and capture output\n",
        "                mem_out = subprocess.check_output(['free', '-g']).decode('utf-8')\n",
        "                print(mem_out)\n",
        "                os_out = subprocess.check_output(['cat', '/etc/os-release']).decode('utf-8')\n",
        "                kernal_out = subprocess.check_output(['uname', '-a']).decode('utf-8')\n",
        "                pyver_out = subprocess.check_output(['which', 'python']).decode('utf-8')\n",
        "                os_out = os_out + \" \\n\" + kernal_out + \"\\n\" + pyver_out\n",
        "                print(os_out)\n",
        "\n",
        "                return_data = {\n",
        "                    \"lscpu_out\": lscpu_out,\n",
        "                    \"mem_out_gb\": mem_out,\n",
        "                    \"os\": os_out\n",
        "                }\n",
        "                return return_data\n",
        "\n",
        "            #\n",
        "            # Start Processing\n",
        "            #\n",
        "            file_bytes = request.files[\"image\"]\n",
        "\n",
        "            # Note that user can define this to be any other type of image\n",
        "            input_image = read_and_rescale_image(file_bytes)\n",
        "\n",
        "            preprocess = transforms.Compose([\n",
        "                xrv.datasets.XRayCenterCrop(),\n",
        "                xrv.datasets.XRayResizer(224)\n",
        "            ])\n",
        "\n",
        "            input_image = preprocess(input_image)\n",
        "            input_batch =  torch.from_numpy( input_image[np.newaxis,...] )\n",
        "\n",
        "            #Benchmark PyTorch\n",
        "            pt_summary = benchmark_pt(input_batch)\n",
        "            print(f\"PyTorch Output: {pt_summary}\")\n",
        "\n",
        "            #Benchmark IPEX\n",
        "            ipex_summary = benchmark_ipex(input_batch)\n",
        "            print(f\"IPEX Output: {ipex_summary}\")\n",
        "\n",
        "            # Benchmark OpenVINO\n",
        "            ov_summary = benchmark_ov(input_batch)\n",
        "            print(f\"OpenVINO Output: {ov_summary}\")\n",
        "\n",
        "            sys_info = get_system_info()\n",
        "\n",
        "            return_data = {\"pt_summary\": pt_summary,\n",
        "            \"ipex_summary\" : ipex_summary,\n",
        "            \"ov_summary\": ov_summary,\n",
        "            \"system_info\": sys_info}\n",
        "\n",
        "            return return_data\n",
        "\n",
        "        except Exception as e:\n",
        "            result = str(e)\n",
        "            # return error message back to the client\n",
        "            return AMLResponse(json.dumps({\"error\": result}), 200)\n",
        "\n",
        "    else:\n",
        "        return AMLResponse(\"bad request\", 500)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Prepare an inference configuration.\n",
        "   * Create an environment object\n",
        "   * Create inference configuration to deploy the model as a web service using:\n",
        "      * The scoring file (`score.py`)\n",
        "         *  Use [AMLRequest](https://docs.microsoft.com/en-us/python/api/azureml-contrib-services/azureml.contrib.services.aml_request?view=azure-ml-py) and [AMLResponse](https://docs.microsoft.com/en-us/python/api/azureml-contrib-services/azureml.contrib.services.aml_response.amlresponse?view=azure-ml-py) classes to access RAW data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create environment object from an environment specification YAML file.\n",
        "See Documentation [HERE](https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py#azureml-core-environment-environment-from-conda-specification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile conda_dep_opti.yml\n",
        "channels:\n",
        "  - anaconda\n",
        "  - defaults\n",
        "dependencies:\n",
        "    - pip:\n",
        "        - azureml-defaults\n",
        "        - azure-ml-api-sdk\n",
        "        - torchxrayvision\n",
        "        - pydicom\n",
        "        - openvino-dev\n",
        "        - torch==1.13.1+cpu\n",
        "        - torchvision==0.14.1+cpu\n",
        "        - intel_extension_for_pytorch==1.13.100\n",
        "        - \"--index-url https://pypi.org/simple/\"\n",
        "        - \"--extra-index-url https://download.pytorch.org/whl/cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from azureml.core.environment import Environment\n",
        "# # We create a light weight environment for inference \n",
        "# # An Environment defines Python packages, environment variables, and Docker settings that are used in machine learning experiments,\n",
        "# # including in data preparation, training, and deployment to a web service.\n",
        "# # Environment Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py\n",
        "# # Conda dependencies Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py\n",
        "# # Conda YAML Documentation: https://learn.microsoft.com/en-us/python/api/azureml-core/azureml.core.environment.environment?view=azure-ml-py#azureml-core-environment-environment-from-conda-specification \n",
        "\n",
        "# # Create environment object from an environment specification YAML file.\n",
        "himms_env_yml = Environment.from_conda_specification('himms_env_opti', 'conda_dep_opti.yml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "import uuid\n",
        "from azureml.core.webservice import Webservice\n",
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core.environment import Environment\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "from azureml.core.environment import CondaDependencies\n",
        "\n",
        "# Connect to workspace\n",
        "from azureml.core import Workspace\n",
        "# Load workspace from config file\n",
        "# The workspace is the top-level resource for Azure Machine Learning, \n",
        "# providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "# Documentation: https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace\n",
        "ws = Workspace.from_config(path='../')\n",
        "print(\"Workspace:\",ws.name)\n",
        "\n",
        "# Register model:\n",
        "# A model is the result of a Azure Machine learning training Run or some other model training process outside of Azure. \n",
        "# Regardless of how the model is produced, it can be registered in a workspace, where it is represented by a name and a version. \n",
        "# With the Model class, you can package models for use with Docker and deploy them as a real-time endpoint that can be used for inference requests.\n",
        "# Please set the version number accordingly the number of models that you have registered.\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py\n",
        "model = Model(ws, 'padchest-pt-onnx-ov', version=1)\n",
        "\n",
        "# Set inference and ACI web service:\n",
        "# The inference configuration describes how to configure the model to make predictions. \n",
        "# It references to the scoring script (entry_script) and is used to locate all the resources required for the deployment. \n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py\n",
        "inference_config = InferenceConfig(entry_script=\"score_opti.py\", environment=himms_env_yml)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 3. Deploy in ACI\n",
        "   Deploy the model as ACI web service. Note that this step may take about 2-5 minutes to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681426521105
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set AciWebservice:\n",
        "# The AciWebservice class represents a machine learning model deployed as a web service endpoint on Azure Container Instances\n",
        "# The Inference configuration (inference_config) is an input parameter for Model deployment-related actions\n",
        "# Note that we trained using a GPU cluster and we set resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=2) respectively.\n",
        "# This will allow us to run inference in CPU and optimize memory. \n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py\n",
        "aci_config = AciWebservice.deploy_configuration(\n",
        "    cpu_cores=2,\n",
        "    memory_gb=4)\n",
        "\n",
        "service_name = 'padchest-opti-sdk-v1'\n",
        "# Deploy:\n",
        "# The model is packaged (using Docker behind the scenes) as a real-time endpoint that is later used for inference requests.\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py\n",
        "service = Model.deploy(workspace=ws, \n",
        "                       name=service_name, \n",
        "                       models=[model], \n",
        "                       inference_config=inference_config, \n",
        "                       deployment_config=aci_config,\n",
        "                       overwrite=True)\n",
        "\n",
        "service.wait_for_deployment(show_output=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681327961407
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# # [Optional] Get deployment service Logs\n",
        "# print(service.get_logs())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 4. Consume data sample and test the web service.\n",
        "We demonstrate how to consume DICOM images:\n",
        "* We trained our model from PNG files with 16 bits pixel depth. \n",
        "* To test the web service, we will send a DICOM file (16 bits).\n",
        "    * We will apply the image normalization implemented in the scoring script.\n",
        "\n",
        "To try out the model you would need a sample DICOM image. In order to obtain one, we recommend that you use one of the PADCHEST images you trained on and use the provided `png2dcm.py` script to generate a DICOM file out of it. You can also try using your own DICOM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Assuming \"sample.png\" file exists. THe following cmd will generate \"sample_dicom.dcm\" file.\n",
        "!python png2dcm.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681425478818
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pydicom\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "# Visualize converted DICOM file from the corresponding PNG file\n",
        "test_file = \"./sample_dicom.dcm\"\n",
        "dcm = pydicom.read_file(test_file)\n",
        "print(dcm)\n",
        "plt.imshow(dcm.pixel_array, cmap=plt.cm.bone)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the model is deployed we can get the scoring web service's HTTP endpoint, which accepts REST client calls. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681425627207
        }
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from azureml.core.webservice import Webservice\n",
        "import numpy as np\n",
        "\n",
        "# Webservice constructor is used to retrieve a cloud representation of a Webservice\n",
        "# object associated with the provided Workspace\n",
        "# Documentation: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice(class)?view=azure-ml-py\n",
        "service = Webservice(name=service_name, workspace=ws)\n",
        "\n",
        "# Get the web service HTTP endpoint.\n",
        "# This endpoint can be shared with anyone who wants to test the web service or integrate it into an application.\n",
        "uri = service.scoring_uri\n",
        "print(uri)\n",
        "\n",
        "files = {'image': open(test_file, 'rb').read()}\n",
        "\n",
        "# Send the DICOM as a raw HTTP request and obtain results from endpoint.\n",
        "response = requests.post(uri, files=files)\n",
        "print(\"output:\", response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681425735538
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "output_dict = json.loads(response.content)\n",
        "\n",
        "pt_metrics = output_dict['pt_summary']\n",
        "ipex_metrics = output_dict['ipex_summary']\n",
        "ov_metrics = output_dict['ov_summary']\n",
        "\n",
        "print(f\"PyTorch Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{pt_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{pt_metrics['pt_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{pt_metrics['pt_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{pt_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{pt_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nIPEX Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{ipex_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{ipex_metrics['ipex_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ipex_metrics['ipex_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ipex_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ipex_metrics['fps']:.2f}\")\n",
        "\n",
        "print(f\"\\nOpenVINO Metrics:\")\n",
        "print(f\"\\tFramework Version:\\t{ov_metrics['fwk_version']}\")\n",
        "print(f\"\\tTop Labels:\\t{ov_metrics['ov_result']['top_labels']}\")\n",
        "print(f\"\\tTop Probabilities:\\t{ov_metrics['ov_result']['top_probabilities']}\")\n",
        "print(f\"\\tAvg Latency:\\t{ov_metrics['avg_latency']:.4f} sec\")\n",
        "print(f\"\\tFPS:\\t{ov_metrics['fps']:.2f}\")\n",
        "\n",
        "# Calculate the FPS speedup with IPEX compared to PyTorch\n",
        "ipex_fps_speedup = ipex_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with IPEX: {ipex_fps_speedup:.2f}x\")\n",
        "\n",
        "# Calculate the FPS speedup with OpenVINO compared to PyTorch\n",
        "ov_fps_speedup = ov_metrics['fps'] / pt_metrics['fps']\n",
        "print(f\"\\nSpeedup with OpenVINO: {ov_fps_speedup:.2f}x\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1681425579906
        }
      },
      "outputs": [],
      "source": [
        "lscpu_out=output_dict['system_info']['lscpu_out'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Info:\\n{lscpu_out}\")\n",
        "\n",
        "mem_out_gb=output_dict['system_info']['mem_out_gb'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem Memory Info:\\n{mem_out_gb}\")\n",
        "\n",
        "os_out=output_dict['system_info']['os'].encode().decode('unicode_escape')\n",
        "print(f\"\\nSystem OS:\\n{os_out}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delete Service Endpoint\n",
        "After testing the service, you can uncomment the following and execute the cell to delete the endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#service.delete()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "6fa67edf6d87aa13ac525a1287441ea8850f1587e23cc2fe3e03f5742d416d61"
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3.8 - AzureML",
      "language": "python",
      "name": "python38-azureml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
